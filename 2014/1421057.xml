<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>III: Small: Large-Scale Structured Sparse Learning</AwardTitle>
    <AwardEffectiveDate>08/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2017</AwardExpirationDate>
    <AwardAmount>333360</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Directorate for Computer &amp; Information Science &amp; Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Christopher Clifton</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Recent technological revolutions have lead to dramatically growing scale, diversity, and complexity of data. Modern data analysis is facing new challenges in handling this complexity. Although complex, the underlying representations of many real-world data are often sparse. This sparseness often exhibits intrinsic structure, e.g., spatial or temporal smoothness, graphs, trees, and groups. Finding effective sparse representations is fundamentally important for scientific discovery; the a-priori structure information may significantly improve the sparse learning model. This project is developing algorithms and tools (including open source software) to enable knowledge discovery from massive high-dimensional and complex data, as well as a new curriculum that incorporates the proposed research into the classroom.&lt;br/&gt;&lt;br/&gt;Most sparse learning algorithms are based on the L1 norm due to its sparsity-inducing property and strong theoretical guarantees, but this does not capture structure. This project is advancing structured sparse learning by (1) analyzing the so-called proximal operators associated with various feature structures, which explains how and why they can induce the desired structured sparsity; (2) developing efficient algorithms for computing the proximal operators, which plays a key building block role in the proposed optimization algorithms; (3) developing a structured sparse learning framework, which includes various sparse learning models and algorithms developed in this project.</AbstractNarration>
    <MinAmdLetterDate>07/22/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>07/22/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1421057</AwardID>
    <Investigator>
      <FirstName>Jieping</FirstName>
      <LastName>Ye</LastName>
      <EmailAddress>jieping.ye@asu.edu</EmailAddress>
      <StartDate>07/22/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Sudhir</FirstName>
      <LastName>Kumar</LastName>
      <EmailAddress>s.kumar@temple.edu</EmailAddress>
      <StartDate>07/22/2014</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Arizona State University</Name>
      <CityName>TEMPE</CityName>
      <ZipCode>852816011</ZipCode>
      <PhoneNumber>4809655479</PhoneNumber>
      <StreetAddress>ORSPA</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Arizona</StateName>
      <StateCode>AZ</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7364</Code>
      <Text>INFO INTEGRATION &amp; INFORMATICS</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7364</Code>
      <Text>INFO INTEGRATION &amp; INFORMATICS</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7923</Code>
      <Text>SMALL PROJECT</Text>
    </ProgramReference>
  </Award>
</rootTag>
