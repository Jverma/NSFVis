<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>EAGER: Discovery of Segmental Sub-Word Structure in Speech</AwardTitle>
    <AwardEffectiveDate>03/01/2014</AwardEffectiveDate>
    <AwardExpirationDate>02/28/2015</AwardExpirationDate>
    <AwardAmount>99911</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Directorate for Computer &amp; Information Science &amp; Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Tatiana D. Korelsky</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This EArly Concept Grant for Exploratory Research (EAGER) investigates new machine learning techniques for discovering sub-word units in speech for use in automatic speech recognition (ASR). The representation of this EArly Concept Grant for Exploratory Research investigates new machine learning techniques for discovering sub-word units in speech for use in automatic speech recognition (ASR). The representation of words in terms of sub-word units is rarely learned from data, despite significant disagreement among linguists as to the sub-word unit inventory. This project represents exploratory work toward a larger goal of making all aspects of ASR learnable, using scientific insights while being discriminatively trained.&lt;br/&gt;&lt;br/&gt;In contrast with prior work, speech segments are clustered into units using discriminatively learned segmental similarities, rather than via dynamic time warping or hidden Markov models. Rather than pre-supposing phoneme-like units, multiple heterogeneous unit types&lt;br/&gt;are learned. The project also leverages multi-modal (video, articulatory, and so on) data to improve unit discovery by sharing&lt;br/&gt;information across modalities. In this exploratory work, the learned units are used in a discriminative model that rescores initial outputs from a standard phone-based recognizer, and the experiments focus on small-/medium-vocabulary recognition.&lt;br/&gt;&lt;br/&gt;This project explores new ways of discovering the basic units of speech. Beyond improvements to speech recognition, this project has&lt;br/&gt;the potential for broad impact on other research areas involving sequences with segmental sub-structure (such as text, video,&lt;br/&gt;biological data, and financial data) or involving clustering. The results may also include new representations for the study of speech&lt;br/&gt;in linguistics and speech science. From a societal perspective, in the long term making speech recognition more learnable will enable&lt;br/&gt;improved porting of the technology to under-served linguistic communities, which do not have the benefit of large data sets or other resources.</AbstractNarration>
    <MinAmdLetterDate>03/04/2014</MinAmdLetterDate>
    <MaxAmdLetterDate>03/04/2014</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1433485</AwardID>
    <Investigator>
      <FirstName>Karen</FirstName>
      <LastName>Livescu</LastName>
      <EmailAddress>klivescu@ttic.edu</EmailAddress>
      <StartDate>03/04/2014</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Toyota Technological Institute at Chicago</Name>
      <CityName>Chicago</CityName>
      <ZipCode>606372902</ZipCode>
      <PhoneNumber>7738340409</PhoneNumber>
      <StreetAddress>6045 S. Kenwood Avenue</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Illinois</StateName>
      <StateCode>IL</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7916</Code>
      <Text>EAGER</Text>
    </ProgramReference>
  </Award>
</rootTag>
